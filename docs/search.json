[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fletcher McConnell",
    "section": "",
    "text": "Hi there! I’m Fletcher. I am a current MEDS (Master’s of Environmental Data Science) student at the Bren School of Environmental Science and Management (UCSB). I have an academic background in environmental science and professional experience working on and under the ocean. Outside of school and work, I love spending time paddling outrigger canoe, hiking in the mountains and anything else that gets me outside. Feel free to learn more about me by exploring around my website."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Fletcher McConnell",
    "section": "Education:",
    "text": "Education:\nBS Environmental Studies, UCSB (2018)\nCurrent Master’s Student at Bren School, UCSB (Master’s of Environmental Data Science)"
  },
  {
    "objectID": "blog/2024-03-09-shark-aggression-analysis/index.html",
    "href": "blog/2024-03-09-shark-aggression-analysis/index.html",
    "title": "Analyzing Which Ocean Activities Are Most Likely to Involve Aggressive Shark Behavior",
    "section": "",
    "text": "I have been around the ocean for the majority of my life and have enjoyed having fun riding waves or exploring beneath them. I have swum with sharks many times and it has always been a positive experience for me. Ever since the first Jaws movie came out, sharks have been perceived as a killing machine, with a blood lust that can only be appeased by taking human lives. In reality, this is so far from the truth. Sharks are incredible animals and, on the rare occasions where they do have a negative interaction with humans, it is often a mistake on the shark’s part. I wanted to display the data to pull away the curtain of uncertainty on where sharks act aggressively. I hope that this post is informative and not scary.\n\n\nMy overall question for this analysis was “During which ocean activities is a person most likely to experience an aggressive shark incident?”. I made 3 plots that each answered a sub-question pertaining to this overall question.\nspider plot –&gt; “Among White Sharks, Tiger Sharks and Bull Sharks, what does the relative distribution of attacks look like among my 5 defined ocean activities?”\nbar plot –&gt; “How do the counts of fatal and non-fatal encounters compare within each activity?”\nline plot –&gt; “What does the trend of the number of annual attacks for surfing, in particular, look like and why might it look like this?”\n\n\n\nThe data that I used for this analysis came from the opendatasoft website and is linked below. Each row in the data frame represented a single documented aggressive shark incident. There were many columns describing aspects of the attack such as location, time of day, activity, species etc. The columns that I used for my analysis were the following:\n“activity” –&gt; column denoting what the person was doing at the time of attack\n“species” –&gt; column denoting the species of shark involved in the attack\n“year” –&gt; column denoting which year the attack occured during\n“fatal_y_n” –&gt; column denoting whether an attack was fatal (Y) or non-fatal (N)\nData Available Here: https://public.opendatasoft.com/explore/dataset/global-shark-attack/table/?flg=en-us&disjunctive.country&disjunctive.area&disjunctive.activity\n\nThis data had many missing values in some of the columns that I used for my analysis. There were also inconsistent naming conventions within columns. I did my best to work with these abnormalities, but there is likely some amount of error in my results."
  },
  {
    "objectID": "blog/2024-03-09-shark-aggression-analysis/index.html#tiger-shark",
    "href": "blog/2024-03-09-shark-aggression-analysis/index.html#tiger-shark",
    "title": "Analyzing Which Ocean Activities Are Most Likely to Involve Aggressive Shark Behavior",
    "section": "Tiger Shark",
    "text": "Tiger Shark\n\n\nCode\n# count the number of Tiger Shark attacks for each activity\nsum(grepl(\"tiger shark\", sharks$species, ignore.case = TRUE) & grepl(\"fishing\", sharks$activity, ignore.case = TRUE))\n\nsum(grepl(\"tiger shark\", sharks$species, ignore.case = TRUE) & grepl(\"surfing|paddle boarding|body boarding\", sharks$activity, ignore.case = TRUE))\n\nsum(grepl(\"tiger shark\", sharks$species, ignore.case = TRUE) & grepl(\"diving\", sharks$activity, ignore.case = TRUE))\n\nsum(grepl(\"tiger shark\", sharks$species, ignore.case = TRUE) & grepl(\"standing|wading|floating\", sharks$activity, ignore.case = TRUE))\n\nsum(grepl(\"tiger shark\", sharks$species, ignore.case = TRUE) & grepl(\"swimming\", sharks$activity, ignore.case = TRUE))"
  },
  {
    "objectID": "blog/2024-03-09-shark-aggression-analysis/index.html#white-shark",
    "href": "blog/2024-03-09-shark-aggression-analysis/index.html#white-shark",
    "title": "Analyzing Which Ocean Activities Are Most Likely to Involve Aggressive Shark Behavior",
    "section": "White Shark",
    "text": "White Shark\n\n\nCode\n# count the number of White Shark attacks for each activity\nsum(grepl(\"white shark\", sharks_spider$species, ignore.case = TRUE) & grepl(\"fishing\", sharks_spider$activity, ignore.case = TRUE))\n\nsum(grepl(\"white shark\", sharks_spider$species, ignore.case = TRUE) & grepl(\"surfing|paddle boarding|body boarding\", sharks_spider$activity, ignore.case = TRUE))\n\nsum(grepl(\"white shark\", sharks_spider$species, ignore.case = TRUE) & grepl(\"diving\", sharks_spider$activity, ignore.case = TRUE))\n\nsum(grepl(\"white shark\", sharks_spider$species, ignore.case = TRUE) & grepl(\"standing|wading|floating\", sharks_spider$activity, ignore.case = TRUE))\n\nsum(grepl(\"white shark\", sharks_spider$species, ignore.case = TRUE) & grepl(\"swimming\", sharks_spider$activity, ignore.case = TRUE))"
  },
  {
    "objectID": "blog/2024-03-09-shark-aggression-analysis/index.html#bull-shark",
    "href": "blog/2024-03-09-shark-aggression-analysis/index.html#bull-shark",
    "title": "Analyzing Which Ocean Activities Are Most Likely to Involve Aggressive Shark Behavior",
    "section": "Bull Shark",
    "text": "Bull Shark\n\n\nCode\n# count the number of Bull Shark attacks for each activity\nsum(grepl(\"bull shark\", sharks_spider$species, ignore.case = TRUE) & grepl(\"fishing\", sharks_spider$activity, ignore.case = TRUE))\n\nsum(grepl(\"bull shark\", sharks_spider$species, ignore.case = TRUE) & grepl(\"surfing|paddle boarding|body boarding\", sharks_spider$activity, ignore.case = TRUE))\n\nsum(grepl(\"bull shark\", sharks_spider$species, ignore.case = TRUE) & grepl(\"diving\", sharks_spider$activity, ignore.case = TRUE))\n\nsum(grepl(\"bull shark\", sharks_spider$species, ignore.case = TRUE) & grepl(\"standing|wading|floating\", sharks_spider$activity, ignore.case = TRUE))\n\nsum(grepl(\"bull shark\", sharks_spider$species, ignore.case = TRUE) & grepl(\"swimming\", sharks_spider$activity, ignore.case = TRUE))\n\n\nI then used the values calculated for each of the three species of sharks to construct a spider plot in order to visualize differences among species. For this plot, my goal was to represent general differences among the 3 species that I chose. I used solid black, dashed red and dotted blue for each separate species. This way, it is colorblind friendly and easy to separate each line from one another. I had started with rings within the spider plot that would represent particular values, however, I chose to remove these in order to simplify the plot. Since I am using relative amounts, values are not necessary and might prove to be more of a distraction than anything. I also chose to add a caption on the bottom with the total number of attacks among all activities for each species. I think that it is important to highlight that not all 3 species have the same number of attacks.\n\n\nCode\n# create data frame for spider plot to draw from (each value scaled up by 2.7)\nsharks_table &lt;- data.frame(\n  species = c(\"max\", \"min\", \"white shark\", \"tiger shark\", \"bull shark\"),\n  fishing = c(100, 0, 77.22, 62.1, 84.24),\n  surfing = c(100, 0, 99.9, 88.83, 63.99),\n  diving = c(100, 0, 50.76, 47.25, 23.49),\n  \"wading\" = c(100, 0, 4.32, 14.04, 20.25),\n  swimming = c(100, 0, 38.07, 57.78, 78.03)\n)\n\n# change species column to index \nrownames(sharks_table) &lt;- sharks_table$species\n\nsharks_table &lt;- sharks_table[,-1]\n\n# adjust the margins of the plot\npar(mar = c(5, 2, 8, 2) + 0.1)\n\n# create spider plot\nradarchart(sharks_table, \n           cglty = 0, # specify grid line type\n           cglcol = \"gray\",\n           pcol = c(1,2,4),\n           plwd = 2,\n           plty = c(1, 2, 3),\n           vlcex = 1.25\n           )\n\n# add title to plot\ntitle(main = \"Proportion of Attacks By Activity for 3 Shark Species\", \n      cex.main = 2,\n      family = \"josefin\")\n\n# add legend\nlegend(\"topright\",\n       legend = c(\"White Shark\", \"Tiger Shark\", \"Bull Shark\"),\n       lty = c(1,2,3),\n       col = c(1, 2, 4),\n       lwd = 2,\n       title = \"Species\")\n\n# add caption beneath plot\ngrid.text(\"* Total number of attacks across all 5 activities are as follows: White Shark - 576, Tiger Shark - 252, Bull Shark - 173 *\",\n          x = 0.5, y = 0.05, \n          just = c(\"center\", \"bottom\"),\n          gp = gpar(fontsize = 10, col = \"gray\"))\n\n\n This visualization gives some interesting insight into how these three species differ in which activities they are involved in. You can see that White Sharks are mostly involved in surfing incidents and Bull Sharks are mostly involved in swimming incidents."
  },
  {
    "objectID": "blog/2023-12-01-oyster-habitat-analysis/index.html",
    "href": "blog/2023-12-01-oyster-habitat-analysis/index.html",
    "title": "Determining Suitable Habitat for Oysters within EEZ regions of California",
    "section": "",
    "text": "Author: Fletcher McConnell\nGithub Reopository: https://github.com/fletcher-m/oyster-habitat\nThe idea of this analysis was to map areas that are suitable to oysters (based on depth and temperature parameters) and then find out which of the west coast economic exclusive zones (EEZ’s) has the greatest potential for oyster fishing, based on total suitable area within each zone.\nBelow are the packages that I used in my analysis:\n\n\nCode\n# load packages\nlibrary(sf)\nlibrary(terra)\nlibrary(here)\nlibrary(dplyr)\nlibrary(tmap)\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(raster)"
  },
  {
    "objectID": "blog/2023-12-01-oyster-habitat-analysis/index.html#finding-suitable-habitat-for-oysters-in-california-eez-boundaries",
    "href": "blog/2023-12-01-oyster-habitat-analysis/index.html#finding-suitable-habitat-for-oysters-in-california-eez-boundaries",
    "title": "Determining Suitable Habitat for Oysters within EEZ regions of California",
    "section": "",
    "text": "Author: Fletcher McConnell\nGithub Reopository: https://github.com/fletcher-m/oyster-habitat\nThe idea of this analysis was to map areas that are suitable to oysters (based on depth and temperature parameters) and then find out which of the west coast economic exclusive zones (EEZ’s) has the greatest potential for oyster fishing, based on total suitable area within each zone.\nBelow are the packages that I used in my analysis:\n\n\nCode\n# load packages\nlibrary(sf)\nlibrary(terra)\nlibrary(here)\nlibrary(dplyr)\nlibrary(tmap)\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(raster)"
  },
  {
    "objectID": "blog/2023-12-01-oyster-habitat-analysis/index.html#finding-suitable-habitat-for-oysters-along-the-california-coast",
    "href": "blog/2023-12-01-oyster-habitat-analysis/index.html#finding-suitable-habitat-for-oysters-along-the-california-coast",
    "title": "Determining Suitable Habitat for Oysters within EEZ regions of California",
    "section": "Finding Suitable Habitat for Oysters Along the California Coast",
    "text": "Finding Suitable Habitat for Oysters Along the California Coast\n\nData Wrangling\nFirst, I was interested in what portions of Califonia’s coastline would be suitable for Oysters. They have an ideal depth range of 0 to 70 meters and ideal temperature range of 11 to 30 degrees Celsius. In order to find where these parameters are met, I used data of annual sea surface temperatures from 2008 to 2012 and then also used bathymetry data of the ocean. These first series of steps involve me doing a bit of data wrangling and making sure that I would be able to stack my depth raster with my temperature raster. I had previously combined all 4 years of temperature data into a raster stack ‘all_sst’.\n\n\nCode\n# find mean SST\nall_sst_mean &lt;- mean(all_sst)\n\n# convert SST data to Celsius\nall_sst_mean &lt;- all_sst_mean - 273.15\n\n# crop depth to match SST raster\ndepth &lt;- crop(depth, all_sst_mean)\n\n# resample depth data to match SST resolution\ndepth &lt;- resample(x = depth, y =  all_sst_mean, method = \"near\")\n\n# check that that depth and SST match in resolution, extent and crs (by stacking them)\ndepth_sst &lt;- c(depth, all_sst_mean)\n\n\n\n\nSetting Raster Values to Align with Temperature and Depth Parameters\nAfter I had my depth and temperature rasters combined, I then needed to filter these rasters to the values that I wanted. As I mentioned above, Oysters like temperature between 11 and 33 degrees Celsius and do best in depths from 0 to 70 meters. After assigning these parameter values, I plotted areas along the coast that meet both of these criteria. The following is what I came up with:\n\n\nCode\n# set range for habitable oyster depth\ndepth_matrix &lt;- matrix(c(-70, 0, 1,\n                    -Inf, -70, NA, 0, Inf, NA),\n                    byrow = TRUE, ncol = 3)\n\ndepth_sst &lt;- classify(depth, depth_matrix)\n\n# set habitable range for oyster temperature\ntemp_matrix &lt;- matrix(c(11, 30, 1,\n                        -Inf, 11, NA, 30, Inf, NA),\n                      byrow = TRUE, ncol = 3)\n\ntemp_sst &lt;- classify(all_sst_mean, temp_matrix)\n\nocean &lt;- stack(x = c(depth_sst, temp_sst))\n\n# find habitat that satisfies both temp range and depth range\nfunction_1 &lt;- function(x, y){\n  ifelse(x ==1 & y == 1, 1, NA)\n}\nsuitable_habitat &lt;- lapp(rast(ocean), fun = function_1)\n\n\n\nThe above plot shows the California coastline. In this plot, I have set values within temperature and depth parameters equal to “1” and combined both rasters. The colored regions represent areas that are within both of the specified depth and temperature regions.\n\n\nOverlaying Suitable Area with EEZ Regions\nNow that I had the area designated for suitable habitat for oysters, I wanted to explore which of the 5 EEZ regions along the west coast contained the most potential for oyster fishing. In order to do this, I needed to find the total suitable area within each EEZ region. Below is a plot showing each of the 5 regions and their corresponding total areas. The regions from North to South are Washington, Oregon, Northern California, Central California and Southern California.\n\nAs you can see, by comparing the plot above with plot showing suitable habitat, all of the EEZ regions contain at least some suitable oyster habitat. Now, I wanted to find out just how much suitable area there was in each region. This way, I would be able to rank each region in terms of potential for oyster farming. This next code section accomplishes exactly that.\n\n\nCode\n# convert eez data into raster format\nwc_eez_raster &lt;- rasterize(wc_eez, depth, field = \"rgn\")\n\n# find area of suitable habitat within each eez\narea &lt;- expanse(suitable_habitat, unit = \"km\", zones = wc_eez_raster)\n\n# join calculate percent of suitable habitat and join back with eez data\ntotal_suitable_area &lt;- merge(wc_eez, area,\n      by.x = \"rgn\",\n      by.y = \"zone\",\n      all.x = TRUE) |&gt; \n  mutate(p_area = (area/area_km2) * 100)\n\ntotal_suitable_area\n\n\n\nThe above is an output after I joined the amount of suitable area (“area”) and percent of total area (“p_area”) to the original EEZ data frame. This is the output that I was looking for. The step left was to visualize the results in order to gain a clearer picture. This first plot represents the total suitable area of each region.\n\nAs you can see, Central California had the most suitable area and Northern California had the least. Now, I will compare this with the percentage of suitable area within each region.\n\nThis plot gives a bit of a different perspective, because it takes into account the size of each region. Washington ranks first with with the percent of area because it has a comparably smaller total area than the rest of the regions. Oregon and Northern California are the only two regions whose ranking remains unchanged in this second plot, ranking second lowest and lowest, respectively."
  },
  {
    "objectID": "blog/2023-12-01-oyster-habitat-analysis/index.html#generalizing-the-process-for-other-species",
    "href": "blog/2023-12-01-oyster-habitat-analysis/index.html#generalizing-the-process-for-other-species",
    "title": "Determining Suitable Habitat for Oysters within EEZ regions of California",
    "section": "Generalizing the Process for Other Species",
    "text": "Generalizing the Process for Other Species\nAfter completing the analysis for oysters, I wanted to create a more general process to map areas for other species. In order to accomplish this, I wrote a function that follows the same basic process that I used above, using the same temperature and depth data. This function accepts the species, minimum depth, maximum depth, minimum temperature and maximum temperature as inputs and generates an area plot as well as a percentage of total EEZ area plot. Below, you can see the function that I created and an example of using the function for Skip jack Tuna. For the Skipjack Tuna, I input the depth range from 0 to 260 meters (-260 to represent below sea level) and the temperature range from 10 to 21 degrees Celsius.\n\n\nCode\n# create general function for other species\nspecies_range_function &lt;- function(species, min_depth, max_depth, min_temp, max_temp) {\n  \n  depth_input &lt;- matrix(c(max_depth, min_depth, 1,\n                    -Inf, max_depth, NA, min_depth, Inf, NA),\n                    byrow = TRUE, ncol = 3)\n  \n  depth_classify &lt;- classify(depth, depth_input)\n  \n  temp_input &lt;- matrix(c(min_temp, max_temp, 1,\n                        -Inf, min_temp, NA, max_temp, Inf, NA),\n                      byrow = TRUE, ncol = 3)\n  \n  temp_classify &lt;- classify(all_sst_mean, temp_input)\n  \n  depth_temp_stack &lt;- stack(x = c(depth_classify, temp_classify))\n  \n  ocean_function &lt;- function(x, y){\n  ifelse(x == 1 & y == 1, 1, NA)\n}\n  oyster_habitat &lt;- lapp(rast(depth_temp_stack), fun = ocean_function)\n  \n  eez_raster &lt;- rasterize(wc_eez, depth, field = \"rgn\") # not sure about this one\n  \n  oyster_area &lt;- expanse(oyster_habitat, unit = \"km\", zones = eez_raster)\n  \n  joined_data &lt;&lt;- merge(wc_eez, oyster_area,\n      by.x = \"rgn\",\n      by.y = \"zone\",\n      all.x = TRUE) |&gt; \n  mutate(p_area = (area/area_km2) * 100)\n  \n  fish_area_plot &lt;- ggplot(joined_data) +\n    geom_sf(aes(fill = area)) +\n    scale_fill_viridis_c() + \n    labs(title = paste(\"Area of Suitable\", species, \"Habitat in EEZ Regions\"), fill = \"Area (km^2)\")\n  \n  print(fish_area_plot)\n  \n  fish_percent_plot &lt;- ggplot(joined_data) +\n    geom_sf(aes(fill = p_area)) +\n    scale_fill_viridis_c() + \n    labs(title = paste(\"Percent of Suitable\", species, \"Habitat of EEZ Regions\"), fill = \"Percent of Total Area\")\n  \n  print(fish_percent_plot)\n}\n\n\n\n\nCode\n# apply created function to Skipjack Tuna\nspecies_range_function(\"Skipjack Tuna\", 0, -260, 10, 21)\n\n\n\n\nAs you can see, the function created new plots that are based on the new temperature and depth parameters that are related to the Skipjack Tuna. This will be able to accept any values that are input, as long as they are included in the original temperature and depth data used."
  },
  {
    "objectID": "blog/2023-12-01-airplane-crash-analysis/index.html",
    "href": "blog/2023-12-01-airplane-crash-analysis/index.html",
    "title": "Airplane Crash Statistical Analyis",
    "section": "",
    "text": "Author: Fletcher McConnell\nGitHub Repository: https://github.com/fletcher-m/airplane_crashes\nData: Airplane Crashes Since 1908. (n.d.). Retrieved December 12, 2023, from https://www.kaggle.com/datasets/saurograndi/airplane-crashes-since-1908\nAs I was looking around at different data sets that I might be able to perform some statistical analysis on, I came across a set that cataloged airplane crashes from 09/17/1908 to 06/08/2009. Although the information was a bit more morbid than I had anticipated, I thought it contained some interesting potential for analysis. I ended up going down a couple of different paths with this data but the main question that I wanted to answer was if there are any discernible patterns in the the airplane crashes. I specifically wanted to check for a seasonality component having to do with the months of the year and also check for patterns in the time of day that the crashes occurred. If you would like to see the complete code for all of my steps, please refer to the GitHub Repository where that is stored (link above).\nThis is what the dataset that I used looked like:\n\nThe main aspect of this data that I liked was the “Date” and “Time” columns. I wanted to perform some sort of time series analysis on this. I was curious if there are any seasonal trends that can be pulled out of this data. It would be interesting to know if certain times of year are more likely for a plane crash than others. I started by making some exploratory plots to see if anything interesting stood out. During this analysis, I used these packages:\n\n\nCode\nlibrary(forecast)\nlibrary(tidyverse)\nlibrary(lterdatasampler)\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(lubridate)\nlibrary(zoo)\nlibrary(fable)\nlibrary(gridExtra)\n\n\nFirst, I wanted to plot the number of crashes per year to get an idea of the general trend. This is what it looked like:\n\nNext, I wanted to see how the number of crashes corresponded to the different months within the year. Maybe there was some sort of cycle that I could analyze later on:\n\nI also plotted the average number of fatalities per year and compared that to the total number of crashes per year. My goal here was to see if there were any areas that had large discrepancies. It didn’t look like there were any and they followed a similar trend, which would be expected:\n\nAfter looking at these preliminary graphs, I decided to continue with my original idea of performing a time series analysis to see if there were any seasonal trends. I had to do a bit of data wrangling here in order to get it into the correct format to perform a decomposition analysis.\n\n\nCode\nairplane_crashes_2 &lt;- airplane_crashes |&gt; \n  select(-Year, -NumCrashesMonth, -Month, -NumCrashesYear)\n\nairplane_crashes_2 &lt;- airplane_crashes_2 |&gt; \n   mutate(yr_mo = yearmonth(Date)) |&gt; \n  group_by(yr_mo) |&gt; \n  summarize(avg_crashes = n()) |&gt; \n  as_tsibble(index = yr_mo)\n\nairplane_crashes_2 %&gt;% \n  autoplot(avg_crashes)\n\n\n\n\nCode\nairplane_crashes_2 &lt;- airplane_crashes_2 |&gt;\n  as_tsibble(index = yr_mo)\n\nairplane_crashes_2 &lt;- fill_gaps(airplane_crashes_2)\n\nairplane_crashes_2 &lt;- airplane_crashes_2 |&gt; \n  mutate(avg_crashes = na.approx(avg_crashes))\n  \ndcmp &lt;- airplane_crashes_2 |&gt; \n  model(STL(avg_crashes))\n \ncomponents(dcmp)\n\n\n\nAfter looking at the decomposition, it looked like the seasonal component plays a very small part in the data. I then plotted an ACF plot to visualize this a little bit differently.\n\n\nCode\nacf(airplane_crashes_2$avg_crashes, lag.max=36)\n\n\n\nAs you can see, each month is correlated with every other month in essentially that same capacity, meaning there is no evidence for a seasonal trend. If there was a seasonal trend, I would expect more of a split between positive and negative correlation occurring every 6 lags or so (half a year).\nEven though there was not evidence of a seasonal trend, I was still curious about whether there were any daily patterns in crashes. Maybe more crashes occur during one time of day and not another. I made another ACF plot to show this relationship. To make this plot, I found the average number of crashes during each hour of the day across the whole time period of the data.\n\n\nCode\nacf(airplane_crash_time$avg_crashes, lag.max=24)\n\n\n\nThere is some seasonality within the time of the day but most of the correlations are withing the 95% confidence interval and, so, are not statistically significant. I compared this with a plot of the number of average number of crashes during each hour of the day and it is as one would expect. There are more crashes from about 8 am to 8pm than the rest of the day. This can be explained because this is the period of time when the most flights are occurring.\nAfter finding out that there aren’t seasonal patterns in this data, I decided to switch from time series into a little bit of regression analysis. I wanted to find out if the proportion of fatalities (to total on board) decreased at all as the years progressed. My thinking behind this was that airplane safety improved with better technology and pilot training improved as well. So, the crashes that did occur might not have been as fatal as earlier flights. The following plot shows the average proportion of fatalities per year.\n\n\nCode\nairplane_fatal &lt;- airplane_crashes |&gt; \n  group_by(Year) |&gt; \n  summarize(AvgPropFatal = mean(Fatalities/Aboard, na.rm = TRUE)) |&gt; \n  filter(Year &gt;= 1919)\n\nggplot(airplane_fatal, aes(x = Year, y = AvgPropFatal)) +\n  geom_line() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(title = \"Average Proportion of Fatalities Per Year\",\n       y = \"Proportion Fatal\")\n\n\n\nJust from looking at this data, it seems like there is a downward trend. I ran a linear regression model to gain a bit more insight.\n\n\nCode\nmodel &lt;- lm(AvgPropFatal ~ Year, data = airplane_fatal)\nsummary(model)\n\n\nThe results showed a slight, although statistically significant trend downward. With every year of change, the fatality proportion decreases by -0.001. It is very small but still is an interesting insight. It is important to mention that the R squared value was 0.30, which indicates that the year only is able explain 30 % of the decline in the proportion of fatality. There could be so many other factors at play here and I was a bit limited with the data that I had. I decided to test one small piece, and that was to include whether or not a plane was a military plane. I ran another linear regression model including this additional variable.\nAfter running this, in an attempt to try to add in another variable that could help explain the data, I added a dummy variable “IsMilitary”. This was taken from a column that had either a value of 1 (military plane) or 0 (non-military plane). After adding this variable into the regression model, the R squared value increased from 0.0036 to 0.022. This increase is so small, but still an interesting finding.\n\n\nCode\nmodel &lt;- lm(PropFatal ~ DaysSinceStart + IsMilitary, data = airplane_military)\nsummary(model)"
  },
  {
    "objectID": "blog/2023-12-01-airplane-crash-analysis/index.html#analysis-of-airplane-crash-statistics-from-1908-to-2009",
    "href": "blog/2023-12-01-airplane-crash-analysis/index.html#analysis-of-airplane-crash-statistics-from-1908-to-2009",
    "title": "Airplane Crash Statistical Analyis",
    "section": "",
    "text": "Author: Fletcher McConnell\nGitHub Repository: https://github.com/fletcher-m/airplane_crashes\nData: Airplane Crashes Since 1908. (n.d.). Retrieved December 12, 2023, from https://www.kaggle.com/datasets/saurograndi/airplane-crashes-since-1908\nAs I was looking around at different data sets that I might be able to perform some statistical analysis on, I came across a set that cataloged airplane crashes from 09/17/1908 to 06/08/2009. Although the information was a bit more morbid than I had anticipated, I thought it contained some interesting potential for analysis. I ended up going down a couple of different paths with this data but the main question that I wanted to answer was if there are any discernible patterns in the the airplane crashes. I specifically wanted to check for a seasonality component having to do with the months of the year and also check for patterns in the time of day that the crashes occurred. If you would like to see the complete code for all of my steps, please refer to the GitHub Repository where that is stored (link above).\nThis is what the dataset that I used looked like:\n\nThe main aspect of this data that I liked was the “Date” and “Time” columns. I wanted to perform some sort of time series analysis on this. I was curious if there are any seasonal trends that can be pulled out of this data. It would be interesting to know if certain times of year are more likely for a plane crash than others. I started by making some exploratory plots to see if anything interesting stood out. During this analysis, I used these packages:\n\n\nCode\nlibrary(forecast)\nlibrary(tidyverse)\nlibrary(lterdatasampler)\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(lubridate)\nlibrary(zoo)\nlibrary(fable)\nlibrary(gridExtra)\n\n\nFirst, I wanted to plot the number of crashes per year to get an idea of the general trend. This is what it looked like:\n\nNext, I wanted to see how the number of crashes corresponded to the different months within the year. Maybe there was some sort of cycle that I could analyze later on:\n\nI also plotted the average number of fatalities per year and compared that to the total number of crashes per year. My goal here was to see if there were any areas that had large discrepancies. It didn’t look like there were any and they followed a similar trend, which would be expected:\n\nAfter looking at these preliminary graphs, I decided to continue with my original idea of performing a time series analysis to see if there were any seasonal trends. I had to do a bit of data wrangling here in order to get it into the correct format to perform a decomposition analysis.\n\n\nCode\nairplane_crashes_2 &lt;- airplane_crashes |&gt; \n  select(-Year, -NumCrashesMonth, -Month, -NumCrashesYear)\n\nairplane_crashes_2 &lt;- airplane_crashes_2 |&gt; \n   mutate(yr_mo = yearmonth(Date)) |&gt; \n  group_by(yr_mo) |&gt; \n  summarize(avg_crashes = n()) |&gt; \n  as_tsibble(index = yr_mo)\n\nairplane_crashes_2 %&gt;% \n  autoplot(avg_crashes)\n\n\n\n\nCode\nairplane_crashes_2 &lt;- airplane_crashes_2 |&gt;\n  as_tsibble(index = yr_mo)\n\nairplane_crashes_2 &lt;- fill_gaps(airplane_crashes_2)\n\nairplane_crashes_2 &lt;- airplane_crashes_2 |&gt; \n  mutate(avg_crashes = na.approx(avg_crashes))\n  \ndcmp &lt;- airplane_crashes_2 |&gt; \n  model(STL(avg_crashes))\n \ncomponents(dcmp)\n\n\n\nAfter looking at the decomposition, it looked like the seasonal component plays a very small part in the data. I then plotted an ACF plot to visualize this a little bit differently.\n\n\nCode\nacf(airplane_crashes_2$avg_crashes, lag.max=36)\n\n\n\nAs you can see, each month is correlated with every other month in essentially that same capacity, meaning there is no evidence for a seasonal trend. If there was a seasonal trend, I would expect more of a split between positive and negative correlation occurring every 6 lags or so (half a year).\nEven though there was not evidence of a seasonal trend, I was still curious about whether there were any daily patterns in crashes. Maybe more crashes occur during one time of day and not another. I made another ACF plot to show this relationship. To make this plot, I found the average number of crashes during each hour of the day across the whole time period of the data.\n\n\nCode\nacf(airplane_crash_time$avg_crashes, lag.max=24)\n\n\n\nThere is some seasonality within the time of the day but most of the correlations are withing the 95% confidence interval and, so, are not statistically significant. I compared this with a plot of the number of average number of crashes during each hour of the day and it is as one would expect. There are more crashes from about 8 am to 8pm than the rest of the day. This can be explained because this is the period of time when the most flights are occurring.\nAfter finding out that there aren’t seasonal patterns in this data, I decided to switch from time series into a little bit of regression analysis. I wanted to find out if the proportion of fatalities (to total on board) decreased at all as the years progressed. My thinking behind this was that airplane safety improved with better technology and pilot training improved as well. So, the crashes that did occur might not have been as fatal as earlier flights. The following plot shows the average proportion of fatalities per year.\n\n\nCode\nairplane_fatal &lt;- airplane_crashes |&gt; \n  group_by(Year) |&gt; \n  summarize(AvgPropFatal = mean(Fatalities/Aboard, na.rm = TRUE)) |&gt; \n  filter(Year &gt;= 1919)\n\nggplot(airplane_fatal, aes(x = Year, y = AvgPropFatal)) +\n  geom_line() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(title = \"Average Proportion of Fatalities Per Year\",\n       y = \"Proportion Fatal\")\n\n\n\nJust from looking at this data, it seems like there is a downward trend. I ran a linear regression model to gain a bit more insight.\n\n\nCode\nmodel &lt;- lm(AvgPropFatal ~ Year, data = airplane_fatal)\nsummary(model)\n\n\nThe results showed a slight, although statistically significant trend downward. With every year of change, the fatality proportion decreases by -0.001. It is very small but still is an interesting insight. It is important to mention that the R squared value was 0.30, which indicates that the year only is able explain 30 % of the decline in the proportion of fatality. There could be so many other factors at play here and I was a bit limited with the data that I had. I decided to test one small piece, and that was to include whether or not a plane was a military plane. I ran another linear regression model including this additional variable.\nAfter running this, in an attempt to try to add in another variable that could help explain the data, I added a dummy variable “IsMilitary”. This was taken from a column that had either a value of 1 (military plane) or 0 (non-military plane). After adding this variable into the regression model, the R squared value increased from 0.0036 to 0.022. This increase is so small, but still an interesting finding.\n\n\nCode\nmodel &lt;- lm(PropFatal ~ DaysSinceStart + IsMilitary, data = airplane_military)\nsummary(model)"
  },
  {
    "objectID": "blog/2023-12-01-airplane-crash-analysis/index.html#overall-conclusion",
    "href": "blog/2023-12-01-airplane-crash-analysis/index.html#overall-conclusion",
    "title": "Airplane Crash Statistical Analyis",
    "section": "Overall Conclusion",
    "text": "Overall Conclusion\nAlthough I didn’t find any conclusive evidence of trends, the data set was limited in some ways. It was not a complete list of all plane crashes. I confirmed this by looking up some major crashes and seeing that they were not displayed in the data. There are so many variables that contribute to whether a plan crashes or not, and many of these are uncontrollable like bird strikes (Winston, n.d.). The time of year / day does not have much impact but there might be cycles occurring over longer time periods (decades). I found that I was limited by the data in the way that it was organized. I would have liked to group by airplane size but this was not noted. I would have also liked to do some analysis on the regions where each crash occurred. There was no standard way that the location data was input, so it would be very difficult to filter for this.\nAlthough it looks like there are a lot of plane crash incidents from this data, modern plane crashes are incredibly rare. If you ever want to reassure yourself, check out a website that shows all of the planes that are currently in flight and see just how many successful flights there are."
  },
  {
    "objectID": "blog/2023-12-01-ocean-data-access/index.html",
    "href": "blog/2023-12-01-ocean-data-access/index.html",
    "title": "Ethical Considerations of Oceanic Data Access",
    "section": "",
    "text": "In this blog post, I attempt to highlight the importance of open access to ocean data. Collecting data from the ocean, especially the deep ocean, is an incredibly costly and risky operation that only a select number of countries around the world are able to undertake. Although only a few countries collect this data, it is essential that lesser able sovereign states can access it in order to make their own decisions regarding climate change, fishing practices and many other commitments that have economic and social ramifications. There are efforts to confront this problem and I try to highlight a few of these."
  },
  {
    "objectID": "blog/2023-12-01-ocean-data-access/index.html#background-in-ocean-data-collection-and-feasibility",
    "href": "blog/2023-12-01-ocean-data-access/index.html#background-in-ocean-data-collection-and-feasibility",
    "title": "Ethical Considerations of Oceanic Data Access",
    "section": "Background in Ocean Data Collection and Feasibility",
    "text": "Background in Ocean Data Collection and Feasibility\nThe ocean has always been a difficult place to work in. Whether you are on its surface or beneath, the dynamics of wind, currents, waves and other factors set ocean-related activities far apart from those in the terrestrial environment. On land, aircraft can quickly create detailed mapping of large areas and access is much easier. The difficult conditions of the ocean, paired with the ocean’s expansive nature, require that a vast number of resources go into collecting data there. It can cost more than US$30,000 per day to operate a modern research vessel, excluding the cost of scientific personnel and the actual research being done. (Lauro et al. 2014). Due to these constraints, there have been only a few countries that have collected the lion’s share of ocean data and, who ultimately control the access to that data. This information is crucial to so many countries so that they can plan for climate change mitigation, prepare for natural disaster impacts and know how to use the ocean’s resources in a sustainable way."
  },
  {
    "objectID": "blog/2023-12-01-ocean-data-access/index.html#south-africa-a-case-study-in-data-access",
    "href": "blog/2023-12-01-ocean-data-access/index.html#south-africa-a-case-study-in-data-access",
    "title": "Ethical Considerations of Oceanic Data Access",
    "section": "South Africa: A Case Study in Data Access",
    "text": "South Africa: A Case Study in Data Access\nAs I mentioned, open access to data regarding the ocean is often limited due to funding restraints or other barriers. Operating without this knowledge can have ramifications for sustainability of resources and the overall contribution to local and global economies and cultures. South Africa can be used as an example of a nation highly reliant on the ocean and its resources but influenced by insufficient access to data. In South Africa, 85% of the mainland ocean territory is within the deep-sea category (Sink et al., n.d.).\n\n\n\nMap showing seafloor depths and the boundaries of South Africa’s continental Exclusive Economic Zone (EEZ). doi:10.1371/journal.pone.0012008.g002\n\n\nInformation about the deep sea in South Africa, as well as other nations, is necessary to understand how increased industrial activities (i.e., oil extraction) may affect marine ecosystems present in that area (Sink et al., n.d.). Additionally, a lack of information could lead to a country missing out on potentially beneficial marine resources (Sink et al., n.d.). One can see how a disparity could arise between a country with enough knowledge to understand its marine territory and one without. Even if two countries had the same resources present in their waters, they must know that these resources exist and the best method for access in order to develop strategies to acquire it. Beyond beneficial natural resources, there are many culturally important aspects of the ocean in South Africa. For example, some Nguni cultures consider many of the most powerful ancestors to reside in the deep sea (Sink et al., n.d.). This cultural aspect is often glossed over in favor of hard data, and this is something that I will touch on later.\nClearly, the ocean landscape is crucially important to South Africa, both for current projects as well as potential future ventures. A major issue there has been that the majority of research in the region has been conducted by international scientists and stakeholders of private companies, with very little to no local collaboration (Sink et al., n.d.). Much of the data collected by private industries was never published or shared with South African interests and some of it was even kept as confidential (Sink et al., n.d.). This is a direct threat to open data access and, as a result, research done by South Africa took an incomplete and fragmented path. This potentially could have slowed innovation in South African deep-sea technology and/or led to damaged ecosystems. Both could potentially have been minimized, had the data been accessible.\nAside from South African entities being unable to access externally collected data, there also have existed issues inside the country involving its citizens being unable to access information from various organizations within their own government. A paper by Sink et al. encapsulates this best by saying that “As a result of past laws of segregation, exclusion and discrimination, multiple sectors including biodiversity, marine science and marine management are still largely inaccessible to the vast majority of South African citizens, and there are significant barriers preventing general access to participation and opportunities” (Sink et al., n.d.). This is evidence that culture can have a direct impact on the progress of science. Although South Africa is perhaps the most well-known for its past policies of racial prejudice, it is by no means unique in this sense. Culture is malleable and these prohibitory measures can be corrected but, for any of this to be fixed, it must first be recognized as a barrier to be addressed. Understanding the history of a country can often help explain many of the systems that are currently in place."
  },
  {
    "objectID": "blog/2023-12-01-ocean-data-access/index.html#what-can-be-done-to-improve-data-access",
    "href": "blog/2023-12-01-ocean-data-access/index.html#what-can-be-done-to-improve-data-access",
    "title": "Ethical Considerations of Oceanic Data Access",
    "section": "What Can Be Done to Improve Data Access?",
    "text": "What Can Be Done to Improve Data Access?\nMany of the data access bottlenecks do not stem from a strict denial of access, but rather an issue of not knowing where the data is kept. The phenomenon of many different organizations holding all different types of ocean data can inhibit data access at the country, state or regional level. The good news is that this is a solvable problem.\nIn 2010, the European Commission pointed out this very issue of marine data access. They indicated that information was held by hundreds of organizations in the EU and that finding out who held that data was a major problem (Shepherd 2018). Finding where the data resided was the first hurdle, which was followed by a lengthy negotiation for access. These procedures made it difficult or impossible to piece different data sets together for a complete picture of any issue that needed to be addressed. Generally, the system was very decentralized, making for an incredibly inefficient process. The solution that the EU came up with was the European Marine Observation and Data Network (EMODnet) (Shepherd 2018). The main idea behind this was that the data should be maintained by organizations that had collected or own the data, but that it should all be accessed in a common way. EMODnet made it possible for a user to search for and retrieve all measurements of a single parameter within any given time and space window, no matter where the data was stored (Shepherd 2018). This was an incredible step in data access for the EU. No longer did someone have to track down a data set, request permission, wait for permission and then sort through the data for an area of interest. Now, all one had to do was specify a few parameters and then click a button.\nThis problem of data access is not limited to the European Union. It is a problem that has been in many parts of the world, with all different types of data. A common thread in the solutions that have been developed is centralization and standardization. It is acceptable for data to be stored in different locations and owned by different entities, but if access is needed, it should be easy to find and retrieve."
  },
  {
    "objectID": "blog/2023-12-01-ocean-data-access/index.html#a-new-framework-for-looking-at-data",
    "href": "blog/2023-12-01-ocean-data-access/index.html#a-new-framework-for-looking-at-data",
    "title": "Ethical Considerations of Oceanic Data Access",
    "section": "A New Framework for Looking at Data",
    "text": "A New Framework for Looking at Data\n\n\n\n[@raising2017]\n\n\nFor a very long time, the term “data” has been reserved for empirical and quantifiable measurements. It is associated with the hard sciences and, in relation to the ocean, it takes the form of sea surface temperature, counts of species, wind speed and other similar measurements. All this collected information is incredibly vast and undeniably important. It is what allows us to conduct commerce, extract valuable resources and, hopefully, protect this natural environment. There is a wealth of policy-focused research on topics like fisheries science and ocean acidification (“Raising and Integrating the Cultural Values of the Ocean | IUCN” 2017). Policy makers are able to take this research into account when they are forming decisions. Something that is often neglected in the formation of these decisions is cultural data. This is something that is beginning to gain traction and is being interwoven into research being done. However, for all the wealth of the available empirical research, there is still an equally insufficient amount of policy relevant documentation that describes the ocean as a cultural entity (“Raising and Integrating the Cultural Values of the Ocean | IUCN” 2017).\nA major challenge in including this cultural data is finding the effective means to document it in a way that can be used for decision-making. Much of this knowledge comes in the form of stories, genealogies, philosophical and ethical systems (“Raising and Integrating the Cultural Values of the Ocean | IUCN” 2017). Unlike empirical data, which can be tabulated and clearly presented in data visualizations, the cultural information cannot be easily counted and run through a computer program. The solution to this is going to involve collaboration between scientists, policymakers and indigenous members. The effort of this collaboration will be to present this information in a way that respects custodianship of this knowledge, while being in a usable format to benefit ocean sustainability policies (“Raising and Integrating the Cultural Values of the Ocean | IUCN” 2017)."
  },
  {
    "objectID": "blog/2023-12-01-ocean-data-access/index.html#summary-and-overall-thoughts",
    "href": "blog/2023-12-01-ocean-data-access/index.html#summary-and-overall-thoughts",
    "title": "Ethical Considerations of Oceanic Data Access",
    "section": "Summary and Overall Thoughts",
    "text": "Summary and Overall Thoughts\nIn this blog, I have focused on data that has to do with the ocean. One could argue that this data is important to every country, no matter what their proximity is to the ocean. Since shipping is the main way that goods are distributed across the globe, every country is tied in some way to marine-derived data. The ideas of data access should, however, be applied to a range of different types of data around the world. I think that a distinction should be made between data ownership and eligibility for access. As I mentioned above, many methods of data collection are quite sophisticated and costly, so it is unreasonable to expect that every country will be able to collect their own data. The way that I see it is that more developed countries have an obligation to assist developing countries with data collection and analysis. It is crucial to note, however, that it is of equal importance that a country should want to be helped. There is vast amount of knowledge that is not currently considered “data” that resides in the oral traditions and rich cultural fabric of countries. This knowledge should be weighted just as much as empirical data because, really, each helps to lift the other up in the end."
  },
  {
    "objectID": "blog/2023-12-01-thomas-fire-analysis/index.html",
    "href": "blog/2023-12-01-thomas-fire-analysis/index.html",
    "title": "Plotting AQI and Thomas Fire Burn Area in Santa Barbara County in Python",
    "section": "",
    "text": "For this project, I looked into constructing a plot of air quality index (AQI) values for Santa Barbara from 2017 to 2018. I also constructed an overlay of the Thomas Fire burn area on a false color image of Santa Barbara county.\nLink to Github Repository: https://github.com/fletcher-m/aqi-thomas-fire-sb\n\n\nThe main goal for this section of the project was to develop a plot in order to visibly see if I could pick out a spike in AQI values during the time of the Thomas Fire in 2017. Fires tend to lead to spikes in AQI values, due to the amount of smoke that they emit. I expected to see that spike sometime around December of 2017.\nThe data that I used for plotting AQI values was AQI data for the years of 2017-2018. If you would like to see what data I used and other coding steps that I took, please check out the GitHub repository linked above. Generally, I combined the data, filtered to the Santa Barbara County and made some updates in order to simplify the data frame. An important note is that I converted the date column to a datetime object in order to plot this. “aqi_sb” is the data frame that I had wrangled in some previous steps.\n\n\nCode\n# update the 'date' column to a datetime object and override the 'date' column with the output\naqi_sb.date = pd.to_datetime(aqi_sb.date)\n\n# set date column as index\naqi_sb = aqi_sb.set_index('date')\n\n# check to make sure the index is the 'date' column --&gt; The data type of the index is 'datetime64'\naqi_sb.index\n\n\nAfter I had the data in the correct format, time frame and target area, all that was left was to plot the information so that I could easily visualize it. I plotted a 5 day average along with daily AQI values to give a better idea of longer trends in the data. Below is the final plot.\n\n\nCode\n# assign line colors for 'aqi' and 'five_day_average' \ncolors = {'aqi': 'red',\n         'five_day_average': 'black'}\n\n# make a line plot for 'aqi' and 'five_day_average' over time\naqi_sb[['aqi', 'five_day_average']].plot(title=\"Daily AQI in Santa Barbara (2017-2018)\",\n                                       color=colors,\n                                        ylabel='AQI')\n\n\n\nAs you can see from the plot, there is an obvious spike around December of 2017, which is what I expected. The Thomas Fire was quite large and left a massive burn area in its wake. In the next section, I will plot that burn area and show it in relation to Santa Barbara County.\n\n\n\nI first set about plotting an image of Santa Barbara using data collected from satellite. There were a couple of steps before this, which you can see on my github repository (link above). This first plot uses the “red”, “green”, and “blue” bands in order to create a true color image. Below, you can see that image plotted with the Santa Barbara region and Channel Islands off of the coast.\n\n\nCode\n# select red, green and blue variables from the NetCDF data and plot \nbands[['red', 'green', 'blue']].to_array().plot.imshow(robust = TRUE)\n\n\n\nAfter plotting this true color image of the Santa Barbara region, I wanted to see what a false color image would look like. This followed the same process, only selecting different bands to display. Instead of “red”, “green” and “blue”, I chose short-wave infrared (SWIR22), near-infrared (NIR08), and red bands. Below is a plot of what that looks like.\n\n\nCode\n# select short wave infrared, near-infrared and red variables and plot\nbands[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust=True)\n\n\n\nIn order to find the burn area for the Thomas Fire (2017), I had to do some filtering of the fire data so that I ended up with only data about the Thomas Fire.\n\n\nCode\n# initiate figure\nfig, ax = plt.subplots(figsize=(6,6))\n\n# plot false color image of Santa Barbara\nfalse_color.plot.imshow(ax=ax, robust=True)\n\n# plot Thomas Fire region\nthomas_fire.plot(ax=ax, color='red', alpha=0.5, edgecolor='black', linewidth=0.5)\nthomas_patch = mpatches.Patch(color='red', label='Thomas Fire')\n\n# create legend\nax.legend(handles=[thomas_patch], frameon=True, loc='upper right', bbox_to_anchor=(1.4, 1))\nax.set_title('False Color Image of Santa Barbara with Thomas Fire Boundary')\n\n\n\n\n\n\nVisualizing data is almost always the best way to gain a grasp on data. Looking at the raw data for AQI values in Santa Barbara would have led to seeing high numbers during the Thomas Fire, but the line plot really puts it into perspective. Overlaying the burn area is also a great way to show the impact of the fire.\nI think that at times, it is easy to get caught up in the data, forgetting where it all came from, so I want to briefly describe the impact of the fire that I analyzed. The Thomas Fire was the largest fire in modern California history, burning 273,400 acres over a period of 6 months (“The Thomas Fire, the Largest Wildfire in California’s Modern History, Is Out | CNN,” n.d.). It was the 3rd most destructive in structure losses, destroying 1,063 buildings (“The Thomas Fire, the Largest Wildfire in California’s Modern History, Is Out | CNN,” n.d.). There were many factors that led to the size of this fire, including strong Santa Ana winds that fanned the flames, and it will undoubtedly remain in the top of the charts for California fires."
  },
  {
    "objectID": "blog/2023-12-01-thomas-fire-analysis/index.html#plotting-aqi-in-santa-barbara",
    "href": "blog/2023-12-01-thomas-fire-analysis/index.html#plotting-aqi-in-santa-barbara",
    "title": "Plotting AQI and Thomas Fire Burn Area in Santa Barbara County in Python",
    "section": "",
    "text": "The main goal for this section of the project was to develop a plot in order to visibly see if I could pick out a spike in AQI values during the time of the Thomas Fire in 2017. Fires tend to lead to spikes in AQI values, due to the amount of smoke that they emit. I expected to see that spike sometime around December of 2017.\nThe data that I used for plotting AQI values was AQI data for the years of 2017-2018. If you would like to see what data I used and other coding steps that I took, please check out the GitHub repository linked above. Generally, I combined the data, filtered to the Santa Barbara County and made some updates in order to simplify the data frame. An important note is that I converted the date column to a datetime object in order to plot this. “aqi_sb” is the data frame that I had wrangled in some previous steps.\n\n\nCode\n# update the 'date' column to a datetime object and override the 'date' column with the output\naqi_sb.date = pd.to_datetime(aqi_sb.date)\n\n# set date column as index\naqi_sb = aqi_sb.set_index('date')\n\n# check to make sure the index is the 'date' column --&gt; The data type of the index is 'datetime64'\naqi_sb.index\n\n\nAfter I had the data in the correct format, time frame and target area, all that was left was to plot the information so that I could easily visualize it. I plotted a 5 day average along with daily AQI values to give a better idea of longer trends in the data. Below is the final plot.\n\n\nCode\n# assign line colors for 'aqi' and 'five_day_average' \ncolors = {'aqi': 'red',\n         'five_day_average': 'black'}\n\n# make a line plot for 'aqi' and 'five_day_average' over time\naqi_sb[['aqi', 'five_day_average']].plot(title=\"Daily AQI in Santa Barbara (2017-2018)\",\n                                       color=colors,\n                                        ylabel='AQI')\n\n\n\nAs you can see from the plot, there is an obvious spike around December of 2017, which is what I expected. The Thomas Fire was quite large and left a massive burn area in its wake. In the next section, I will plot that burn area and show it in relation to Santa Barbara County."
  },
  {
    "objectID": "blog/2023-12-01-thomas-fire-analysis/index.html#plotting-image-of-santa-barbara-with-thomas-fire-boundary",
    "href": "blog/2023-12-01-thomas-fire-analysis/index.html#plotting-image-of-santa-barbara-with-thomas-fire-boundary",
    "title": "Plotting AQI and Thomas Fire Burn Area in Santa Barbara County in Python",
    "section": "",
    "text": "I first set about plotting an image of Santa Barbara using data collected from satellite. There were a couple of steps before this, which you can see on my github repository (link above). This first plot uses the “red”, “green”, and “blue” bands in order to create a true color image. Below, you can see that image plotted with the Santa Barbara region and Channel Islands off of the coast.\n\n\nCode\n# select red, green and blue variables from the NetCDF data and plot \nbands[['red', 'green', 'blue']].to_array().plot.imshow(robust = TRUE)\n\n\n\nAfter plotting this true color image of the Santa Barbara region, I wanted to see what a false color image would look like. This followed the same process, only selecting different bands to display. Instead of “red”, “green” and “blue”, I chose short-wave infrared (SWIR22), near-infrared (NIR08), and red bands. Below is a plot of what that looks like.\n\n\nCode\n# select short wave infrared, near-infrared and red variables and plot\nbands[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust=True)\n\n\n\nIn order to find the burn area for the Thomas Fire (2017), I had to do some filtering of the fire data so that I ended up with only data about the Thomas Fire.\n\n\nCode\n# initiate figure\nfig, ax = plt.subplots(figsize=(6,6))\n\n# plot false color image of Santa Barbara\nfalse_color.plot.imshow(ax=ax, robust=True)\n\n# plot Thomas Fire region\nthomas_fire.plot(ax=ax, color='red', alpha=0.5, edgecolor='black', linewidth=0.5)\nthomas_patch = mpatches.Patch(color='red', label='Thomas Fire')\n\n# create legend\nax.legend(handles=[thomas_patch], frameon=True, loc='upper right', bbox_to_anchor=(1.4, 1))\nax.set_title('False Color Image of Santa Barbara with Thomas Fire Boundary')"
  },
  {
    "objectID": "blog/2023-12-01-thomas-fire-analysis/index.html#summary-and-take-aways",
    "href": "blog/2023-12-01-thomas-fire-analysis/index.html#summary-and-take-aways",
    "title": "Plotting AQI and Thomas Fire Burn Area in Santa Barbara County in Python",
    "section": "",
    "text": "Visualizing data is almost always the best way to gain a grasp on data. Looking at the raw data for AQI values in Santa Barbara would have led to seeing high numbers during the Thomas Fire, but the line plot really puts it into perspective. Overlaying the burn area is also a great way to show the impact of the fire.\nI think that at times, it is easy to get caught up in the data, forgetting where it all came from, so I want to briefly describe the impact of the fire that I analyzed. The Thomas Fire was the largest fire in modern California history, burning 273,400 acres over a period of 6 months (“The Thomas Fire, the Largest Wildfire in California’s Modern History, Is Out | CNN,” n.d.). It was the 3rd most destructive in structure losses, destroying 1,063 buildings (“The Thomas Fire, the Largest Wildfire in California’s Modern History, Is Out | CNN,” n.d.). There were many factors that led to the size of this fire, including strong Santa Ana winds that fanned the flames, and it will undoubtedly remain in the top of the charts for California fires."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’ve spent most of my life around the ocean and, for a few years growing up, sailing on it. When I was 7, my family (parents, sister, cat & dog) decided to buy a sailboat and go travel for a while. We began in Ventura, CA and slowly worked our way down the coast of California and down into Mexico and then Central America for 2 years. We ended up leaving the sailboat in El Salvador and moving to Lake Tahoe for the next 2 years. After enjoying the mountains and learning how to ski, we decided that we wanted to give travelling another shot. We moved back down onto the sailboat and traveled for another couple years, completing Central America and seeing some of northern South America. Our final leg consisted of us heading out to the Galapagos Islands and then back up to Mexico from there. Altogether, we were living full time on the boat for 5 years.\n\n\n\n\n\n\n\n\nMy family’s boat “Desiderata” that we lived on for 5 years."
  },
  {
    "objectID": "about.html#growing-up-on-the-water",
    "href": "about.html#growing-up-on-the-water",
    "title": "About",
    "section": "",
    "text": "I’ve spent most of my life around the ocean and, for a few years growing up, sailing on it. When I was 7, my family (parents, sister, cat & dog) decided to buy a sailboat and go travel for a while. We began in Ventura, CA and slowly worked our way down the coast of California and down into Mexico and then Central America for 2 years. We ended up leaving the sailboat in El Salvador and moving to Lake Tahoe for the next 2 years. After enjoying the mountains and learning how to ski, we decided that we wanted to give travelling another shot. We moved back down onto the sailboat and traveled for another couple years, completing Central America and seeing some of northern South America. Our final leg consisted of us heading out to the Galapagos Islands and then back up to Mexico from there. Altogether, we were living full time on the boat for 5 years.\n\n\n\n\n\n\n\n\nMy family’s boat “Desiderata” that we lived on for 5 years."
  },
  {
    "objectID": "about.html#daily-life-on-the-boat",
    "href": "about.html#daily-life-on-the-boat",
    "title": "About",
    "section": "Daily Life on the Boat",
    "text": "Daily Life on the Boat\nMany people hear that we lived on a sailboat and assume that it was like being on a vacation every day. In reality, our lifestyle required some hard work. We dealt with all of the same trials and tribulations of any family, and all in within a 41 foot boat. My sister and I were home schooled (or boat schooled I suppose) and we had weekly chores, just like other kids. The difference was that, after cleaning the bathrooms, we could walk up and jump right in the water.\n\n\n\n\n\n\n\n\nStanding on Machu Picchu - one of the many amazing sights that we saw.\n\nOther areas of life on a boat are still possible, but much less convenient than if you were living in a house. For example, going to buy groceries was no longer as simple as hopping in a car and running out to the store. We were normally anchored offshore from a town, so our first step was to take our inflatable boat through the waves and haul it up onto the beach. We then needed to catch a bus that would take us to the store (sometimes it would be a half hour ride). After buying everything that we needed (or at least all that we could carry), we would make the trek back to the boat. Stocking up on groceries often turned into an all-day affair for us.\nTravelling by sailboat has now become very popular. Every year, hundreds of boats leave from San Diego on their way to Mexico in an event called the “Baja Ha Ha”. It’s a great way for first-time sailors/travelers to meet others doing the same thing and have some support along the way. You can check out my family’s full website that documents our travels if you’re interested to learn more."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Projects",
    "section": "",
    "text": "Analyzing Which Ocean Activities Are Most Likely to Involve Aggressive Shark Behavior\n\n\n\n\n\n\n\nOcean\n\n\nMEDS\n\n\nSharks\n\n\nVisualization\n\n\n\n\nThis is an analysis that conducted, where I looked at data containing accounts of aggressive shark behavior across the globe going all the way back to 1638 through 2023. I wanted to find out which activities people were doing when a shark became aggressive towards them.\n\n\n\n\n\n\nMar 9, 2024\n\n\nFletcher McConnell\n\n\n\n\n\n\n  \n\n\n\n\nEthical Considerations of Oceanic Data Access\n\n\n\n\n\n\n\nQuarto\n\n\nMEDS\n\n\nOcean\n\n\n\n\nI analyze bottlenecks in the flow of information and important ethical consideratons in regards to ocean data\n\n\n\n\n\n\nDec 11, 2023\n\n\nFletcher McConnell\n\n\n\n\n\n\n  \n\n\n\n\nDetermining Suitable Habitat for Oysters within EEZ regions of California\n\n\n\n\n\n\n\nQuarto\n\n\nMEDS\n\n\nOcean\n\n\nVisualization\n\n\n\n\nIn this post, I analyze suitable habitat for oysters, based on temperature and depth parameters. I then find the amount of area within California EEZ regions that meets these requirements\n\n\n\n\n\n\nDec 1, 2023\n\n\nFletcher McConnell\n\n\n\n\n\n\n  \n\n\n\n\nAirplane Crash Statistical Analyis\n\n\n\n\n\n\n\nQuarto\n\n\nMEDS\n\n\nWorkshop\n\n\n\n\nIn this blog, I analyze various aspects of airplane crashes, using a a dataset that logs crashes from 1908 to 2009\n\n\n\n\n\n\nDec 1, 2023\n\n\nFletcher McConnell\n\n\n\n\n\n\n  \n\n\n\n\nPlotting AQI and Thomas Fire Burn Area in Santa Barbara County in Python\n\n\n\n\n\n\n\nQuarto\n\n\nMEDS\n\n\nFire\n\n\nVisualization\n\n\n\n\nThis is an analysis that I did where I looked at daily air quality index values in Santa Barbara in 2017-2018 as well as the area burned by the Thomas Fire (2017) and plotted both.\n\n\n\n\n\n\nDec 1, 2023\n\n\nFletcher McConnell\n\n\n\n\n\n\nNo matching items"
  }
]