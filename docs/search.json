[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fletcher McConnell",
    "section": "",
    "text": "Hi, I’m Fletcher.\nI am a current MEDS (Master’s of Environmental Data Science) student at the Bren School of Environmental Science and Management. I have spent most of my life either on or near the ocean, so my interests tend to pull me in that direction."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Fletcher McConnell",
    "section": "Education:",
    "text": "Education:\nBS Environmental Studies (2018)\nCurrent Master’s Student at Bren, UCSB (Master’s of Environmental Data Science)"
  },
  {
    "objectID": "blog/2023-10-23-my-first-post/index.html",
    "href": "blog/2023-10-23-my-first-post/index.html",
    "title": "My first blog post",
    "section": "",
    "text": "Here is some1 awesome text."
  },
  {
    "objectID": "blog/2023-10-23-my-first-post/index.html#this-is-my-first-section",
    "href": "blog/2023-10-23-my-first-post/index.html#this-is-my-first-section",
    "title": "My first blog post",
    "section": "",
    "text": "Here is some1 awesome text."
  },
  {
    "objectID": "blog/2023-10-23-my-first-post/index.html#this-is-my-second-section",
    "href": "blog/2023-10-23-my-first-post/index.html#this-is-my-second-section",
    "title": "My first blog post",
    "section": "This is my second section",
    "text": "This is my second section\nHello.2\nI read stuff at this resource(Csik 2022).\nI’m citing another resource here(Gaynor et al. 2022)."
  },
  {
    "objectID": "blog/2023-10-23-my-first-post/index.html#footnotes",
    "href": "blog/2023-10-23-my-first-post/index.html#footnotes",
    "title": "My first blog post",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is my first footnote↩︎\nThis is an inline footnote↩︎"
  },
  {
    "objectID": "blog/2023-12-01-oyster-habitat-analysis/index.html",
    "href": "blog/2023-12-01-oyster-habitat-analysis/index.html",
    "title": "Determining Suitable Habitat for Oysters within EEZ regions of California",
    "section": "",
    "text": "Author: Fletcher McConnell\nGithub Reopository: https://github.com/fletcher-m/oyster-habitat\nThe idea of this analysis was to map areas that are suitable to oysters (based on depth and temperature parameters) and then find out which of the west coast economic exclusive zones (EEZ’s) has the greatest potential for oyster fishing, based on total suitable area within each zone.\nBelow are the packages that I used in my analysis:\n\n\nCode\n# load packages\nlibrary(sf)\nlibrary(terra)\nlibrary(here)\nlibrary(dplyr)\nlibrary(tmap)\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(raster)"
  },
  {
    "objectID": "blog/2023-12-01-oyster-habitat-analysis/index.html#finding-suitable-habitat-for-oysters-in-california-eez-boundaries",
    "href": "blog/2023-12-01-oyster-habitat-analysis/index.html#finding-suitable-habitat-for-oysters-in-california-eez-boundaries",
    "title": "Determining Suitable Habitat for Oysters within EEZ regions of California",
    "section": "",
    "text": "Author: Fletcher McConnell\nGithub Reopository: https://github.com/fletcher-m/oyster-habitat\nThe idea of this analysis was to map areas that are suitable to oysters (based on depth and temperature parameters) and then find out which of the west coast economic exclusive zones (EEZ’s) has the greatest potential for oyster fishing, based on total suitable area within each zone.\nBelow are the packages that I used in my analysis:\n\n\nCode\n# load packages\nlibrary(sf)\nlibrary(terra)\nlibrary(here)\nlibrary(dplyr)\nlibrary(tmap)\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(raster)"
  },
  {
    "objectID": "blog/2023-12-01-oyster-habitat-analysis/index.html#finding-suitable-habitat-for-oysters-along-the-california-coast",
    "href": "blog/2023-12-01-oyster-habitat-analysis/index.html#finding-suitable-habitat-for-oysters-along-the-california-coast",
    "title": "Determining Suitable Habitat for Oysters within EEZ regions of California",
    "section": "Finding Suitable Habitat for Oysters Along the California Coast",
    "text": "Finding Suitable Habitat for Oysters Along the California Coast\n\nData Wrangling\nFirst, I was interested in what portions of Califonia’s coastline would be suitable for Oysters. They have an ideal depth range of 0 to 70 meters and ideal temperature range of 11 to 30 degrees Celsius. In order to find where these parameters are met, I used data of annual sea surface temperatures from 2008 to 2012 and then also used bathymetry data of the ocean. These first series of steps involve me doing a bit of data wrangling and making sure that I would be able to stack my depth raster with my temperature raster. I had previously combined all 4 years of temperature data into a raster stack ‘all_sst’.\n\n\nCode\n# find mean SST\nall_sst_mean &lt;- mean(all_sst)\n\n# convert SST data to Celsius\nall_sst_mean &lt;- all_sst_mean - 273.15\n\n# crop depth to match SST raster\ndepth &lt;- crop(depth, all_sst_mean)\n\n# resample depth data to match SST resolution\ndepth &lt;- resample(x = depth, y =  all_sst_mean, method = \"near\")\n\n# check that that depth and SST match in resolution, extent and crs (by stacking them)\ndepth_sst &lt;- c(depth, all_sst_mean)\n\n\n\n\nSetting Raster Values to Align with Temperature and Depth Parameters\nAfter I had my depth and temperature rasters combined, I then needed to filter these rasters to the values that I wanted. As I mentioned above, Oysters like temperature between 11 and 33 degrees Celsius and do best in depths from 0 to 70 meters. After assigning these parameter values, I plotted areas along the coast that meet both of these criteria. The following is what I came up with:\n\n\nCode\n# set range for habitable oyster depth\ndepth_matrix &lt;- matrix(c(-70, 0, 1,\n                    -Inf, -70, NA, 0, Inf, NA),\n                    byrow = TRUE, ncol = 3)\n\ndepth_sst &lt;- classify(depth, depth_matrix)\n\n# set habitable range for oyster temperature\ntemp_matrix &lt;- matrix(c(11, 30, 1,\n                        -Inf, 11, NA, 30, Inf, NA),\n                      byrow = TRUE, ncol = 3)\n\ntemp_sst &lt;- classify(all_sst_mean, temp_matrix)\n\nocean &lt;- stack(x = c(depth_sst, temp_sst))\n\n# find habitat that satisfies both temp range and depth range\nfunction_1 &lt;- function(x, y){\n  ifelse(x ==1 & y == 1, 1, NA)\n}\nsuitable_habitat &lt;- lapp(rast(ocean), fun = function_1)\n\n\n\nThe above plot shows the California coastline. In this plot, I have set values within temperature and depth parameters equal to “1” and combined both rasters. The colored regions represent areas that are within both of the specified depth and temperature regions.\n\n\nOverlaying Suitable Area with EEZ Regions\nNow that I had the area designated for suitable habitat for oysters, I wanted to explore which of the 5 EEZ regions along the west coast contained the most potential for oyster fishing. In order to do this, I needed to find the total suitable area within each EEZ region. Below is a plot showing each of the 5 regions and their corresponding total areas. The regions from North to South are Washington, Oregon, Northern California, Central California and Southern California.\n\nAs you can see, by comparing the plot above with plot showing suitable habitat, all of the EEZ regions contain at least some suitable oyster habitat. Now, I wanted to find out just how much suitable area there was in each region. This way, I would be able to rank each region in terms of potential for oyster farming. This next code section accomplishes exactly that.\n\n\nCode\n# convert eez data into raster format\nwc_eez_raster &lt;- rasterize(wc_eez, depth, field = \"rgn\")\n\n# find area of suitable habitat within each eez\narea &lt;- expanse(suitable_habitat, unit = \"km\", zones = wc_eez_raster)\n\n# join calculate percent of suitable habitat and join back with eez data\ntotal_suitable_area &lt;- merge(wc_eez, area,\n      by.x = \"rgn\",\n      by.y = \"zone\",\n      all.x = TRUE) |&gt; \n  mutate(p_area = (area/area_km2) * 100)\n\ntotal_suitable_area\n\n\n\nThe above is an output after I joined the amount of suitable area (“area”) and percent of total area (“p_area”) to the original EEZ data frame. This is the output that I was looking for. The step left was to visualize the results in order to gain a clearer picture. This first plot represents the total suitable area of each region.\n\nAs you can see, Central California had the most suitable area and Northern California had the least. Now, I will compare this with the percentage of suitable area within each region.\n\nThis plot gives a bit of a different perspective, because it takes into account the size of each region. Washington ranks first with with the percent of area because it has a comparably smaller total area than the rest of the regions. Oregon and Northern California are the only two regions whose ranking remains unchanged in this second plot, ranking second lowest and lowest, respectively."
  },
  {
    "objectID": "blog/2023-12-01-oyster-habitat-analysis/index.html#generalizing-the-process-for-other-species",
    "href": "blog/2023-12-01-oyster-habitat-analysis/index.html#generalizing-the-process-for-other-species",
    "title": "Determining Suitable Habitat for Oysters within EEZ regions of California",
    "section": "Generalizing the Process for Other Species",
    "text": "Generalizing the Process for Other Species\nAfter completing the analysis for oysters, I wanted to create a more general process to map areas for other species. In order to accomplish this, I wrote a function that follows the same basic process that I used above, using the same temperature and depth data. This function accepts the species, minimum depth, maximum depth, minimum temperature and maximum temperature as inputs and generates an area plot as well as a percentage of total EEZ area plot. Below, you can see the function that I created and an example of using the function for Skip jack Tuna.\n\n\nCode\n# create general function for other species\nspecies_range_function &lt;- function(species, min_depth, max_depth, min_temp, max_temp) {\n  \n  depth_input &lt;- matrix(c(max_depth, min_depth, 1,\n                    -Inf, max_depth, NA, min_depth, Inf, NA),\n                    byrow = TRUE, ncol = 3)\n  \n  depth_classify &lt;- classify(depth, depth_input)\n  \n  temp_input &lt;- matrix(c(min_temp, max_temp, 1,\n                        -Inf, min_temp, NA, max_temp, Inf, NA),\n                      byrow = TRUE, ncol = 3)\n  \n  temp_classify &lt;- classify(all_sst_mean, temp_input)\n  \n  depth_temp_stack &lt;- stack(x = c(depth_classify, temp_classify))\n  \n  ocean_function &lt;- function(x, y){\n  ifelse(x == 1 & y == 1, 1, NA)\n}\n  oyster_habitat &lt;- lapp(rast(depth_temp_stack), fun = ocean_function)\n  \n  eez_raster &lt;- rasterize(wc_eez, depth, field = \"rgn\") # not sure about this one\n  \n  oyster_area &lt;- expanse(oyster_habitat, unit = \"km\", zones = eez_raster)\n  \n  joined_data &lt;&lt;- merge(wc_eez, oyster_area,\n      by.x = \"rgn\",\n      by.y = \"zone\",\n      all.x = TRUE) |&gt; \n  mutate(p_area = (area/area_km2) * 100)\n  \n  fish_area_plot &lt;- ggplot(joined_data) +\n    geom_sf(aes(fill = area)) +\n    scale_fill_viridis_c() + \n    labs(title = paste(\"Area of Suitable\", species, \"Habitat in EEZ Regions\"), fill = \"Area (km^2)\")\n  \n  print(fish_area_plot)\n  \n  fish_percent_plot &lt;- ggplot(joined_data) +\n    geom_sf(aes(fill = p_area)) +\n    scale_fill_viridis_c() + \n    labs(title = paste(\"Percent of Suitable\", species, \"Habitat of EEZ Regions\"), fill = \"Percent of Total Area\")\n  \n  print(fish_percent_plot)\n}\n\n\n\n\nCode\n# apply created function to Skipjack Tuna\nspecies_range_function(\"Skipjack Tuna\", 0, -260, 10, 21)\n\n\n\n\nAs you can see, the function created new plots that are based on the new temperature and depth parameters that are related to the Skipjack Tuna. This will be able to accept any values that are input, as long as they are included in the original temperature and depth data used."
  },
  {
    "objectID": "blog/2023-12-01-airplane-crash-analysis/index.html",
    "href": "blog/2023-12-01-airplane-crash-analysis/index.html",
    "title": "EDS 222 Final Project",
    "section": "",
    "text": "CitationBibTeX citation:@online{mcconnell2023,\n  author = {McConnell, Fletcher},\n  title = {EDS 222 {Final} {Project}},\n  date = {2023-12-01},\n  url = {https://fletcher-m.github.io/blog/2023-12-01-eds222-final-project/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMcConnell, Fletcher. 2023. “EDS 222 Final Project.”\nDecember 1, 2023. https://fletcher-m.github.io/blog/2023-12-01-eds222-final-project/."
  },
  {
    "objectID": "blog/2023-12-01-ocean-data-access/index.html",
    "href": "blog/2023-12-01-ocean-data-access/index.html",
    "title": "Ethical Considerations of Oceanic Data Access",
    "section": "",
    "text": "In this blog post, I attempt to highlight the importance of open access to ocean data. Collecting data from the ocean, especially the deep ocean, is an incredibly costly and risky operation that only a select number of countries around the world are able to undertake. Although only a few countries collect this data, it is essential that lesser able sovereign states can access it in order to make their own decisions regarding climate change, fishing practices as well as many other commitments that have economic and social ramifications. There are efforts to confront this problem and I try to highlight a few of these."
  },
  {
    "objectID": "blog/2023-12-01-ocean-data-access/index.html#background-in-ocean-data-collection-and-feasibility",
    "href": "blog/2023-12-01-ocean-data-access/index.html#background-in-ocean-data-collection-and-feasibility",
    "title": "Ethical Considerations of Oceanic Data Access",
    "section": "Background in Ocean Data Collection and Feasibility",
    "text": "Background in Ocean Data Collection and Feasibility\nThe ocean has always been a difficult place to work in. Whether you are on its surface or beneath, the dynamics of wind, currents, waves and other factors set ocean-related activities far apart from those in the terrestrial environment. On land, aircraft can quickly create detailed mapping of large areas and access is much easier. The difficult conditions of the ocean, paired with the ocean’s expansive nature, require that a vast number of resources go into collecting data there. It can cost more than US$30,000 per day to operate a modern research vessel, excluding the cost of scientific personnel and the actual research being done. (Lauro et al. 2014). Due to these constraints, there have been only a few countries that have collected the lion’s share of ocean data and, who ultimately control the access to that data. This data is crucial to so many countries so that they can plan for mitigating climate change, prepare for natural disaster impacts and know how to use the ocean’s resources in a sustainable way."
  },
  {
    "objectID": "blog/2023-12-01-ocean-data-access/index.html#south-africa-a-case-study-in-data-access",
    "href": "blog/2023-12-01-ocean-data-access/index.html#south-africa-a-case-study-in-data-access",
    "title": "Ethical Considerations of Oceanic Data Access",
    "section": "South Africa: A Case Study in Data Access",
    "text": "South Africa: A Case Study in Data Access\nAs I mentioned, open access to data regarding the ocean is often limited due to funding restraints or other barriers to access. Operating without this knowledge can have ramifications for sustainability of resources and the overall contribution to local and global economies and cultures. South Africa can be used as an example of a nation highly reliant on the ocean and its resources but influenced by insufficient access to data. In South Africa, 85% of the mainland ocean territory is within the deep-sea category (Sink et al., n.d.).\n\n\n\nMap showing seafloor depths and the boundaries of South Africa’s continental Exclusive Economic Zone (EEZ). doi:10.1371/journal.pone.0012008.g002\n\n\nInformation about the deep sea in South Africa, as well as other nations, is necessary to understand how increased industrial activities (i.e., oil extraction) may affect marine ecosystems present in that area (Sink et al., n.d.). Additionally, a lack of information could lead to a country missing out on potentially beneficial marine resources (Sink et al., n.d.). One can see how a disparity could arise between a country with deep knowledge of its marine territory and one without. Even if two countries had the same resources present in their waters, they must know that resource exists in order to develop strategies to acquire it. Beyond beneficial natural resources, there are many culturally important aspects of the deep sea in South Africa. For example, some Nguni cultures consider some of the most powerful ancestors to reside in the deep sea (Sink et al., n.d.).\nClearly, the ocean landscape is crucially important to South Africa, both for current projects as well as potential future ventures. A major issue there has been that the majority of research in that region has been conducted by international scientists and stakeholders of private companies, with very little to no local collaboration (Sink et al., n.d.). Much of the data collected by private industries was never published or shared with South African interests and some of it was even kept as confidential (Sink et al., n.d.). This is a direct threat to open data access and, as a result, research done by South Africa took an incomplete and fragmented path. This potentially could have slowed innovation in South African deep-sea technology and/or led to damaged ecosystems. Both could potentially have been minimized, had the data been accessible.\nAside from South African entities being unable to access externally collected data, there also have existed issues inside the country of its citizens being unable to access information from various organizations within the government. A paper by Sink et. al. encapsulates this best by saying that “As a result of past laws of segregation, exclusion and discrimination, multiple sectors including biodiversity, marine science and marine management are still largely inaccessible to the vast majority of South African citizens, and there are significant barriers preventing general access to participation and opportunities” (Sink et al., n.d.). This is evidence that culture can have a direct impact on the progress of science. Although South Africa is perhaps the most well-known for its past policies of racial prejudice, it is by no means unique in this sense. Culture is malleable and these prohibitory measures can be corrected but, for any of this to be fixed, it must first be recognized as a barrier to be addressed."
  },
  {
    "objectID": "blog/2023-12-01-ocean-data-access/index.html#what-can-be-done-to-improve-data-access",
    "href": "blog/2023-12-01-ocean-data-access/index.html#what-can-be-done-to-improve-data-access",
    "title": "Ethical Considerations of Oceanic Data Access",
    "section": "What Can Be Done to Improve Data Access?",
    "text": "What Can Be Done to Improve Data Access?\nMany of the data access bottlenecks do not stem from a strict denial of access, but rather an issue of not knowing where the data is kept. This issue of many different organizations holding all different types of ocean data can inhibit data access at the country, state or regional level. The good news is that this is a solvable problem.\nIn 2010, the European Commission pointed out this very issue of marine data access. They indicated that information was held by hundreds of organizations in the EU and that finding out who held that data was a major problem (Shepherd 2018). Finding where the data resided was the first hurdle, which was followed by a lengthy negotiation for access. These procedures made it difficult or impossible to piece different data sets together for a complete picture of an issue to be addressed. All in all, the system was very decentralized, making for an incredibly inefficient process. The solution that the EU came up with was the European Marine Observation and Data Network (EMODnet) (Shepherd 2018). The main idea behind this was that the data should be maintained by organizations that had collected or own the data, but that it should all be accessed in a common way. EMODnet made it possible for a user to search for and retrieve all measurements of a single parameter within any given time and space window, no matter where the data was stored (Shepherd 2018). This was an incredible step in data access for the EU. No longer did someone have to track down a data set, request permission, wait for permission and then sort through the data for an area of interest. Now, all one had to do was specify a few parameters and then click a button.\nThis problem of data access is not limited to the European Union. It is a problem that has been in many parts of the world, with all different types of data. A common thread in the solutions that have been developed is centralization and standardization. It is ok for data to be stored in different locations and owned by different entities, but if access is needed, it should be easy to find and retrieve."
  },
  {
    "objectID": "blog/2023-12-01-ocean-data-access/index.html#a-new-framework-for-looking-at-data",
    "href": "blog/2023-12-01-ocean-data-access/index.html#a-new-framework-for-looking-at-data",
    "title": "Ethical Considerations of Oceanic Data Access",
    "section": "A New Framework for Looking at Data",
    "text": "A New Framework for Looking at Data\n\n\n\n[@raising2017]\n\n\nFor a very long time, the term “data” has been reserved for empirical and quantifiable measurements. It is associated with the hard sciences and, in relation to the ocean, it takes the form of sea surface temperature, counts of species, wind speed and other similar measurements. All of this collected information is incredibly vast and undeniably important. It is what allows us to conduct commerce, extract valuable resources and, hopefully, protect this natural environment. There is a wealth of policy-focused research on fisheries science and ocean acidification (“Raising and Integrating the Cultural Values of the Ocean | IUCN” 2017). Policy makers are able to take this research into account when they are forming decisions. Something that is often neglected in the formation of these decisions is cultural data. This is something that is beginning to gain traction and is being interwoven into research being done. However, for all the wealth of the available empirical research, there is an equally insufficient amount of policy relevant documentation that describes the ocean as a cultural entity (“Raising and Integrating the Cultural Values of the Ocean | IUCN” 2017).\nA big challenge is finding the effective means to document cultural data in a way that can be used for decision-making. Much of this information comes in the form of stories, genealogies, philosophical and ethical systems (“Raising and Integrating the Cultural Values of the Ocean | IUCN” 2017). Unlike empirical data, which can be tabulated and clearly presented in data visualizations, the cultural information cannot be easily counted and run through a computer program. The solution to this is going to involve collaboration between scientists, policymakers and indigenous members. The effort of this collaboration will be to present this information in a way that respects custodianship of this knowledge, while being in a usable format to benefit ocean sustainability policies (“Raising and Integrating the Cultural Values of the Ocean | IUCN” 2017)."
  },
  {
    "objectID": "blog/2023-12-01-ocean-data-access/index.html#summary-and-overall-thoughts",
    "href": "blog/2023-12-01-ocean-data-access/index.html#summary-and-overall-thoughts",
    "title": "Ethical Considerations of Oceanic Data Access",
    "section": "Summary and Overall Thoughts",
    "text": "Summary and Overall Thoughts\nIn this blog, I have focused on data that has to do with the ocean. One could argue that this data is important to every country, no matter what their proximity is to the ocean. Since shipping is the main way that goods are distributed across the globe, every country is tied in some way to marine-derived data. The ideas of data access should, however, be applied to a range of different types of data around the world. I think that a distinction should be made between data ownership and eligibility for access. As I mentioned above, many methods of data collection are quite sophisticated and costly, so it is unreasonable to expect that every country will be able to collect their own data. The way that I see it is that more developed countries have an obligation to assist developing countries with data collection and analysis. It is crucial to note, however, that it is of equal importance that a country should want to be helped. There is vast amount of knowledge that is not currently considered “data” that resides in the oral traditions and rich cultural fabric of countries. This knowledge should be weighted just as much as empirical data because, really, each helps to lift the other up in the end."
  },
  {
    "objectID": "blog/2023-12-01-thomas-fire-analysis/index.html",
    "href": "blog/2023-12-01-thomas-fire-analysis/index.html",
    "title": "Plotting AQI and Thomas Fire Burn Area in Santa Barbara County in Python",
    "section": "",
    "text": "For this project, I looked into constructing a plot of air quality index (AQI) values for Santa Barbara from 2017 to 2018. I also constructed an overlay of the Thomas Fire burn area on a false color image of Santa Barbara county.\nLink to Github Repository: https://github.com/fletcher-m/aqi-thomas-fire-sb\n\n\nThe main goal for this section of the project was to develop a plot in order to visibly see if I could pick out a spike in AQI values during the time of the Thomas Fire in 2017. Fires tend to lead to spikes in AQI values, due to the amount of smoke that they emit. I expected to see that spike sometime around December of 2017.\nThe data that I used for plotting AQI values was AQI data for the years of 2017-2018. If you would like to see what data I used and other coding steps that I took, please check out the GitHub repository linked above. Generally, I combined the data, filtered to the Santa Barbara County and made some updates in order to simplify the data frame. An important note is that I converted the date column to a datetime object in order to plot this. “aqi_sb” is the data frame that I had wrangled in some previous steps.\n\n\nCode\n# update the 'date' column to a datetime object and override the 'date' column with the output\naqi_sb.date = pd.to_datetime(aqi_sb.date)\n\n# set date column as index\naqi_sb = aqi_sb.set_index('date')\n\n# check to make sure the index is the 'date' column --&gt; The data type of the index is 'datetime64'\naqi_sb.index\n\n\nAfter I had the data in the correct format, time frame and target area, all that was left was to plot the information so that I could easily visualize it. I plotted a 5 day average along with daily AQI values to give a better idea of longer trends in the data. Below is the final plot.\n\n\nCode\n# assign line colors for 'aqi' and 'five_day_average' \ncolors = {'aqi': 'red',\n         'five_day_average': 'black'}\n\n# make a line plot for 'aqi' and 'five_day_average' over time\naqi_sb[['aqi', 'five_day_average']].plot(title=\"Daily AQI in Santa Barbara (2017-2018)\",\n                                       color=colors,\n                                        ylabel='AQI')\n\n\n\nAs you can see from the plot, there is an obvious spike around December of 2017, which is what I expected. The Thomas Fire was quite large and left a massive burn area in its wake. In the next section, I will plot that burn area and show it in relation to Santa Barbara County.\n\n\n\nI first set about plotting an image of Santa Barbara using data collected from satellite. There were a couple of steps before this, which you can see on my github repository (link above). This first plot uses the “red”, “green”, and “blue” bands in order to create a true color image. Below, you can see that image plotted with the Santa Barbara region and Channel Islands off of the coast.\n\n\nCode\n# select red, green and blue variables from the NetCDF data and plot \nbands[['red', 'green', 'blue']].to_array().plot.imshow(robust = TRUE)\n\n\n\nAfter plotting this true color image of the Santa Barbara region, I wanted to see what a false color image would look like. This followed the same process, only selecting different bands to display. Instead of “red”, “green” and “blue”, I chose short-wave infrared (SWIR22), near-infrared (NIR08), and red bands. Below is a plot of what that looks like.\n\n\nCode\n# select short wave infrared, near-infrared and red variables and plot\nbands[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust=True)\n\n\n\nIn order to find the burn area for the Thomas Fire (2017), I had to do some filtering of the fire data so that I ended up with only data about the Thomas Fire.\n\n\nCode\n# initiate figure\nfig, ax = plt.subplots(figsize=(6,6))\n\n# plot false color image of Santa Barbara\nfalse_color.plot.imshow(ax=ax, robust=True)\n\n# plot Thomas Fire region\nthomas_fire.plot(ax=ax, color='red', alpha=0.5, edgecolor='black', linewidth=0.5)\nthomas_patch = mpatches.Patch(color='red', label='Thomas Fire')\n\n# create legend\nax.legend(handles=[thomas_patch], frameon=True, loc='upper right', bbox_to_anchor=(1.4, 1))\nax.set_title('False Color Image of Santa Barbara with Thomas Fire Boundary')\n\n\n\n\n\n\nVisualizing data is almost always the best way to gain a grasp on data. Looking at the raw data for AQI values in Santa Barbara would have led to seeing high numbers during the Thomas Fire, but the line plot really puts it into perspective. Overlaying the burn area is also a great way to show the impact of the fire.\nI think that at times, it is easy to get caught up in the data, forgetting where it all came from, so I want to briefly describe the impact of the fire that I analyzed. The Thomas Fire was the largest fire in modern California history, burning 273,400 acres over a period of 6 months (“The Thomas Fire, the Largest Wildfire in California’s Modern History, Is Out | CNN,” n.d.). It was the 3rd most destructive in structure losses, destroying 1,063 buildings (“The Thomas Fire, the Largest Wildfire in California’s Modern History, Is Out | CNN,” n.d.). There were many factors that led to the size of this fire, including strong Santa Ana winds that fanned the flames, and it will undoubtedly remain in the top of the charts for California fires."
  },
  {
    "objectID": "blog/2023-12-01-thomas-fire-analysis/index.html#plotting-aqi-in-santa-barbara",
    "href": "blog/2023-12-01-thomas-fire-analysis/index.html#plotting-aqi-in-santa-barbara",
    "title": "Plotting AQI and Thomas Fire Burn Area in Santa Barbara County in Python",
    "section": "",
    "text": "The main goal for this section of the project was to develop a plot in order to visibly see if I could pick out a spike in AQI values during the time of the Thomas Fire in 2017. Fires tend to lead to spikes in AQI values, due to the amount of smoke that they emit. I expected to see that spike sometime around December of 2017.\nThe data that I used for plotting AQI values was AQI data for the years of 2017-2018. If you would like to see what data I used and other coding steps that I took, please check out the GitHub repository linked above. Generally, I combined the data, filtered to the Santa Barbara County and made some updates in order to simplify the data frame. An important note is that I converted the date column to a datetime object in order to plot this. “aqi_sb” is the data frame that I had wrangled in some previous steps.\n\n\nCode\n# update the 'date' column to a datetime object and override the 'date' column with the output\naqi_sb.date = pd.to_datetime(aqi_sb.date)\n\n# set date column as index\naqi_sb = aqi_sb.set_index('date')\n\n# check to make sure the index is the 'date' column --&gt; The data type of the index is 'datetime64'\naqi_sb.index\n\n\nAfter I had the data in the correct format, time frame and target area, all that was left was to plot the information so that I could easily visualize it. I plotted a 5 day average along with daily AQI values to give a better idea of longer trends in the data. Below is the final plot.\n\n\nCode\n# assign line colors for 'aqi' and 'five_day_average' \ncolors = {'aqi': 'red',\n         'five_day_average': 'black'}\n\n# make a line plot for 'aqi' and 'five_day_average' over time\naqi_sb[['aqi', 'five_day_average']].plot(title=\"Daily AQI in Santa Barbara (2017-2018)\",\n                                       color=colors,\n                                        ylabel='AQI')\n\n\n\nAs you can see from the plot, there is an obvious spike around December of 2017, which is what I expected. The Thomas Fire was quite large and left a massive burn area in its wake. In the next section, I will plot that burn area and show it in relation to Santa Barbara County."
  },
  {
    "objectID": "blog/2023-12-01-thomas-fire-analysis/index.html#plotting-image-of-santa-barbara-with-thomas-fire-boundary",
    "href": "blog/2023-12-01-thomas-fire-analysis/index.html#plotting-image-of-santa-barbara-with-thomas-fire-boundary",
    "title": "Plotting AQI and Thomas Fire Burn Area in Santa Barbara County in Python",
    "section": "",
    "text": "I first set about plotting an image of Santa Barbara using data collected from satellite. There were a couple of steps before this, which you can see on my github repository (link above). This first plot uses the “red”, “green”, and “blue” bands in order to create a true color image. Below, you can see that image plotted with the Santa Barbara region and Channel Islands off of the coast.\n\n\nCode\n# select red, green and blue variables from the NetCDF data and plot \nbands[['red', 'green', 'blue']].to_array().plot.imshow(robust = TRUE)\n\n\n\nAfter plotting this true color image of the Santa Barbara region, I wanted to see what a false color image would look like. This followed the same process, only selecting different bands to display. Instead of “red”, “green” and “blue”, I chose short-wave infrared (SWIR22), near-infrared (NIR08), and red bands. Below is a plot of what that looks like.\n\n\nCode\n# select short wave infrared, near-infrared and red variables and plot\nbands[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust=True)\n\n\n\nIn order to find the burn area for the Thomas Fire (2017), I had to do some filtering of the fire data so that I ended up with only data about the Thomas Fire.\n\n\nCode\n# initiate figure\nfig, ax = plt.subplots(figsize=(6,6))\n\n# plot false color image of Santa Barbara\nfalse_color.plot.imshow(ax=ax, robust=True)\n\n# plot Thomas Fire region\nthomas_fire.plot(ax=ax, color='red', alpha=0.5, edgecolor='black', linewidth=0.5)\nthomas_patch = mpatches.Patch(color='red', label='Thomas Fire')\n\n# create legend\nax.legend(handles=[thomas_patch], frameon=True, loc='upper right', bbox_to_anchor=(1.4, 1))\nax.set_title('False Color Image of Santa Barbara with Thomas Fire Boundary')"
  },
  {
    "objectID": "blog/2023-12-01-thomas-fire-analysis/index.html#summary-and-take-aways",
    "href": "blog/2023-12-01-thomas-fire-analysis/index.html#summary-and-take-aways",
    "title": "Plotting AQI and Thomas Fire Burn Area in Santa Barbara County in Python",
    "section": "",
    "text": "Visualizing data is almost always the best way to gain a grasp on data. Looking at the raw data for AQI values in Santa Barbara would have led to seeing high numbers during the Thomas Fire, but the line plot really puts it into perspective. Overlaying the burn area is also a great way to show the impact of the fire.\nI think that at times, it is easy to get caught up in the data, forgetting where it all came from, so I want to briefly describe the impact of the fire that I analyzed. The Thomas Fire was the largest fire in modern California history, burning 273,400 acres over a period of 6 months (“The Thomas Fire, the Largest Wildfire in California’s Modern History, Is Out | CNN,” n.d.). It was the 3rd most destructive in structure losses, destroying 1,063 buildings (“The Thomas Fire, the Largest Wildfire in California’s Modern History, Is Out | CNN,” n.d.). There were many factors that led to the size of this fire, including strong Santa Ana winds that fanned the flames, and it will undoubtedly remain in the top of the charts for California fires."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Determining Suitable Habitat for Oysters within EEZ regions of California\n\n\n\n\n\n\n\nQuarto\n\n\nMEDS\n\n\nOcean\n\n\nVisualization\n\n\n\n\nIn this post, I analyze suitable habitat for oysters, based on temperature and depth parameters. I then find the amount of area within California EEZ regions that meets these requirements\n\n\n\n\n\n\nDec 1, 2023\n\n\nFletcher McConnell\n\n\n\n\n\n\n  \n\n\n\n\nEDS 222 Final Project\n\n\n\n\n\n\n\nQuarto\n\n\nMEDS\n\n\nWorkshop\n\n\n\n\nThis is the final project for eds 222\n\n\n\n\n\n\nDec 1, 2023\n\n\nFletcher McConnell\n\n\n\n\n\n\n  \n\n\n\n\nEthical Considerations of Oceanic Data Access\n\n\n\n\n\n\n\nQuarto\n\n\nMEDS\n\n\nOcean\n\n\n\n\nI analyze bottlenecks in the flow of information and important ethical consideratons in regards to ocean data\n\n\n\n\n\n\nDec 1, 2023\n\n\nFletcher McConnell\n\n\n\n\n\n\n  \n\n\n\n\nPlotting AQI and Thomas Fire Burn Area in Santa Barbara County in Python\n\n\n\n\n\n\n\nQuarto\n\n\nMEDS\n\n\nFire\n\n\nVisualization\n\n\n\n\nThis is an analysis that I did where I looked at daily air quality index values in Santa Barbara in 2017-2018 as well as the area burned by the Thomas Fire (2017) and plotted both.\n\n\n\n\n\n\nDec 1, 2023\n\n\nFletcher McConnell\n\n\n\n\n\n\n  \n\n\n\n\nMy first blog post\n\n\n\n\n\n\n\nQuarto\n\n\nMEDS\n\n\nWorkshop\n\n\n\n\nI’m going to tell you about my blog\n\n\n\n\n\n\nOct 23, 2023\n\n\nFletcher McConnell\n\n\n\n\n\n\nNo matching items"
  }
]